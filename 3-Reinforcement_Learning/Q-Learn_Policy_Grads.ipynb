{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681b4fce",
   "metadata": {},
   "source": [
    "# 1 - Comparing and reinforcing learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ade871",
   "metadata": {},
   "source": [
    "compare the performance and characteristics of two key reinforcement learning algorithms—Q-learning and policy gradients. You will then perform an analysis to reinforce your understanding of how these algorithms work, when they are most effective, and how they handle learning tasks differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ab361",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e304a4",
   "metadata": {},
   "source": [
    "Implement Q-learning\n",
    "The first part of the activity focuses on Q-learning, a value-based reinforcement learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cc8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid size and actions\n",
    "grid_size = 5\n",
    "n_actions = 4  # Actions: up, down, left, right\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "Q_table = np.zeros((grid_size * grid_size, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb1c7e",
   "metadata": {},
   "source": [
    "Step 2: Define the hyperparameters\n",
    "Set the hyperparameters for Q-learning:\n",
    "\n",
    "Learning rate (αα)\n",
    "\n",
    "Discount factor (γγ)\n",
    "\n",
    "Exploration rate (ϵϵ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60434e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor for future rewards\n",
    "epsilon = 0.1  # Exploration rate for epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7beb73",
   "metadata": {},
   "source": [
    "Step 3: Define the reward structure\n",
    "Create a reward matrix based on the environment's feedback:\n",
    "\n",
    "+10 for reaching the goal\n",
    "\n",
    "–10 for falling into the pit\n",
    "\n",
    "–1 for other states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea91af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward matrix for the grid environment\n",
    "rewards = np.full((grid_size * grid_size,), -1)  # -1 for every state\n",
    "rewards[24] = 10  # Goal state\n",
    "rewards[12] = -10  # Pitfall state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c23c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q_table, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, n_actions)  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])  # Exploit: action with highest Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4690f",
   "metadata": {},
   "source": [
    "Update the Q-values\n",
    "Use the Bellman equation to update the Q-values based on the current state, the selected action, and the reward obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091a996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1000):\n",
    "    state = np.random.randint(0, grid_size * grid_size)  # Start in a random state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(Q_table, state, epsilon)\n",
    "        next_state = np.random.randint(0, grid_size * grid_size)  # Simulated next state\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Update Q-value using Bellman equation\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "\n",
    "        state = next_state\n",
    "        if next_state == 24 or next_state == 12:\n",
    "            done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2ae69",
   "metadata": {},
   "source": [
    "# 2 - Implement policy gradients\n",
    "The second part of the activity involves implementing policy gradients, a policy-based method in which the agent learns a policy (mapping from states to actions) by optimizing a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f87bb",
   "metadata": {},
   "source": [
    "Step-by-step guide:\n",
    "Step 1: Build the policy network\n",
    "Define a neural network that takes the current state as input and then outputs a probability distribution over possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd12f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 11:09:43.524895: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-03 11:09:43.708090: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-03 11:09:43.838019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748948983.965704   30588 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748948983.999670   30588 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748948984.243724   30588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748948984.243762   30588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748948984.243766   30588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748948984.243769   30588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-03 11:09:44.272615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/envs/ai-ml/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-03 11:09:49.338856: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the policy network\n",
    "n_states = grid_size * grid_size  # Number of states in the grid\n",
    "n_actions = 4  # Up, down, left, right\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(24, activation='relu', input_shape=(n_states,)),\n",
    "    tf.keras.layers.Dense(n_actions, activation='softmax')  # Output action probabilities\n",
    "])\n",
    "\n",
    "# Optimizer for policy network updates\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97ab24",
   "metadata": {},
   "source": [
    "Step 2: Select an action\n",
    "For each state, the agent selects an action based on the probabilities output by the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24f36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    state_input = tf.one_hot(state, n_states)  # One-hot encoding for state\n",
    "    action_probs = model(state_input[np.newaxis, :])\n",
    "    return np.random.choice(n_actions, p=action_probs.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d6971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation loop\n",
    "states = []\n",
    "actions = []\n",
    "episode_rewards = []  \n",
    "\n",
    "for episode in range(1000):\n",
    "    state = np.random.randint(0, n_states)  # Start in a random state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = get_action(state)  # Use the provided function\n",
    "        next_state = np.random.randint(0, n_states)  # Simulated next state\n",
    "        reward = rewards[next_state]  \n",
    "\n",
    "        # Store the state-action-reward trajectory\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        episode_rewards.append(reward)  \n",
    "\n",
    "        state = next_state\n",
    "        if next_state in {24, 12}:  \n",
    "            done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fedea",
   "metadata": {},
   "source": [
    "Step 4: Compute cumulative rewards\n",
    "To reinforce actions that lead to long-term success, calculate the cumulative rewards for each action taken during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff5dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_rewards(rewards, gamma=0.99):\n",
    "    cumulative_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        cumulative_rewards[t] = running_add\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66504ed1",
   "metadata": {},
   "source": [
    "Step 5: Update the policy\n",
    "Update the policy network using the REINFORCE algorithm based on the actions and cumulative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44bde1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(states, actions, rewards):\n",
    "    cumulative_rewards = compute_cumulative_rewards(rewards)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        state_inputs = tf.one_hot(states, n_states)  # Convert states to one-hot encoding\n",
    "        action_probs = model(state_inputs)\n",
    "        action_masks = tf.one_hot(actions, n_actions)  # Mask for selected actions\n",
    "        log_probs = tf.reduce_sum(action_masks * tf.math.log(action_probs), axis=1)\n",
    "\n",
    "        # Policy loss is the negative log-probability of the action times the cumulative reward\n",
    "        loss = -tf.reduce_mean(log_probs * cumulative_rewards)\n",
    "\n",
    "    # Apply gradients to update the policy network\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f889d98",
   "metadata": {},
   "source": [
    "Step 4: Compute cumulative rewards\n",
    "To reinforce actions that lead to long-term success, calculate the cumulative rewards for each action taken during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2ff6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_rewards(rewards, gamma=0.99):\n",
    "    cumulative_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        cumulative_rewards[t] = running_add\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f1a4c",
   "metadata": {},
   "source": [
    "Step 5: Update the policy\n",
    "Update the policy network using the REINFORCE algorithm based on the actions and cumulative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1b43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(states, actions, rewards):\n",
    "    cumulative_rewards = compute_cumulative_rewards(rewards)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        state_inputs = tf.one_hot(states, n_states)  # Convert states to one-hot encoding\n",
    "        action_probs = model(state_inputs)\n",
    "        action_masks = tf.one_hot(actions, n_actions)  # Mask for selected actions\n",
    "        log_probs = tf.reduce_sum(action_masks * tf.math.log(action_probs), axis=1)\n",
    "\n",
    "        # Policy loss is the negative log-probability of the action times the cumulative reward\n",
    "        loss = -tf.reduce_mean(log_probs * cumulative_rewards)\n",
    "\n",
    "    # Apply gradients to update the policy network\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab705b4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards_q_learning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Example code to visualize rewards over episodes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m plt.plot(\u001b[43mrewards_q_learning\u001b[49m, label=\u001b[33m'\u001b[39m\u001b[33mQ-Learning\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m plt.plot(rewards_policy_gradients, label=\u001b[33m'\u001b[39m\u001b[33mPolicy Gradients\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpisodes\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'rewards_q_learning' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example code to visualize rewards over episodes\n",
    "plt.plot(rewards_q_learning, label='Q-Learning')\n",
    "plt.plot(rewards_policy_gradients, label='Policy Gradients')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Rewards')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e19348",
   "metadata": {},
   "source": [
    "Comparing Q-learning and policy gradients\n",
    "In reinforcement learning, Q-learning and policy gradients are two popular approaches for training agents to make optimal decisions. While both methods aim to maximize cumulative rewards, they differ in their underlying mechanisms and are suited for different types of problems. Compare the methods below.\n",
    "\n",
    "Speed of convergence\n",
    "Q-learning tended to converge faster in this small grid environment. This is because Q-learning works well in environments with a discrete action space and fewer states, allowing the agent to build a reliable Q-table quickly.\n",
    "\n",
    "Policy gradients required more episodes to stabilize because the agent learned the policy directly through gradient updates. However, policy gradients are more flexible in environments with continuous action spaces.\n",
    "\n",
    "Reward maximization\n",
    "Both algorithms eventually reached the goal consistently after enough episodes. However, Q-learning was more consistent in terms of reward maximization early on due to its more structured exploration.\n",
    "\n",
    "Policy gradients started slowly but eventually caught up and produced comparable results.\n",
    "\n",
    "Exploration vs. exploitation\n",
    "Q-learning relies heavily on exploration through the epsilon-greedy policy. The agent systematically explored different paths, but it risked getting stuck in suboptimal actions when epsilon was too high.\n",
    "\n",
    "Policy gradients did not explicitly balance exploration and exploitation; instead, they optimized the policy based on cumulative rewards, which naturally led to better action selection as the policy improved.\n",
    "\n",
    "Suitability for different problems\n",
    "Q-learning is more suited to environments with a small number of discrete actions and states, such as grid-based games or simple navigation tasks.\n",
    "\n",
    "Policy gradients are better suited for environments with a continuous action space or more complex scenarios where approximating a value function (such as Q-values) becomes difficult.\n",
    "\n",
    "Conclusion\n",
    "In this walkthrough, you implemented and compared Q-learning and policy gradient algorithms in a simple grid environment. Both approaches demonstrated their strengths: Q-learning’s faster convergence and policy gradients' flexibility. While Q-learning is easier to implement in smaller, discrete environments, policy gradients offer a more scalable solution for larger, continuous action spaces.\n",
    "\n",
    "By comparing both algorithms, you now have a deeper understanding of when to use each approach based on the complexity of the problem and the type of action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a15e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
