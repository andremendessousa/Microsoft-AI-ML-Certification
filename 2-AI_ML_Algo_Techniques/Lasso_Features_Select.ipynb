{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72b27dd",
   "metadata": {},
   "source": [
    "# 1 - Teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcaba59",
   "metadata": {},
   "source": [
    "Feature selection methods: Backward elimination, forward selection, and LASSO\n",
    "Introduction\n",
    "Feature selection is an essential part of building efficient machine learning models. By selecting the most relevant features, you can improve model performance, reduce overfitting, and enhance interpretability. \n",
    "\n",
    "This reading will describe three common techniques for feature selection: backward elimination, forward selection, and least absolute shrinkage and selection operator (LASSO). These methods help identify which features are the most significant for a given model and discard irrelevant ones.\n",
    "\n",
    "By the end of this reading, you'll be able to:\n",
    "\n",
    "Explain how backward elimination removes less significant features, improving model performance.\n",
    "\n",
    "Apply forward selection to incrementally add significant features to a model.\n",
    "\n",
    "Implement LASSO to automatically select important features through regularization.\n",
    "\n",
    "Backward elimination\n",
    "Backward elimination is a feature selection technique that starts with all the available features and progressively removes the least significant features one by one. The goal is to eliminate features that do not contribute much to the predictive power of a given model.\n",
    "\n",
    "Steps of backward elimination\n",
    "Fit the model—e.g., linear regression—with all the features in the dataset.\n",
    "\n",
    "Calculate p-values to determine how statistically significant each feature is.\n",
    "\n",
    "Remove the least significant feature—i.e., the feature with the highest p-value. \n",
    "\n",
    "Repeat the process with the remaining features until all remaining features are statistically significant—i.e., below a predefined significance level, typically 0.05.\n",
    "\n",
    "Advantages\n",
    "Straightforward and intuitive.\n",
    "\n",
    "Works well when there are many irrelevant features.\n",
    "\n",
    "Disadvantages\n",
    "Can be computationally expensive for large datasets.\n",
    "\n",
    "May remove features that are important in combination with others but seem irrelevant when considered individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2aa9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample data: X is the feature matrix, y is the target variable\n",
    "X = sm.add_constant(X)  # Add a constant (intercept) to the model\n",
    "model = sm.OLS(y, X).fit()  # Fit an Ordinary Least Squares regression\n",
    "print(model.summary())  # Display the model summary\n",
    "\n",
    "# Backward elimination: remove the feature with the highest p-value and refit the model\n",
    "# Repeat the process until all remaining features have a p-value < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926bf52",
   "metadata": {},
   "source": [
    "Forward selection\n",
    "Forward selection is the opposite of backward elimination. Instead of starting with all features, forward selection begins with no features and adds them one by one based on their statistical significance and impact on model performance.\n",
    "\n",
    "Steps of forward selection\n",
    "Start with an empty model: Begin with no features.\n",
    "\n",
    "Add the most significant feature: Add the feature that has the highest correlation with the target variable or provides the most improvement to the model.\n",
    "\n",
    "Refit the model: After each feature is added, refit the model and evaluate the performance, e.g., using adjusted R-squared or another metric.\n",
    "\n",
    "Repeat: Continue adding features until the addition of further features no longer improves the model’s performance.\n",
    "\n",
    "Advantages\n",
    "Useful when there are many features as it builds the model step by step\n",
    "\n",
    "Computationally less expensive than backward elimination for very large datasets\n",
    "\n",
    "Disadvantages\n",
    "May include features that only appear significant due to their relationship with other features\n",
    "\n",
    "Slower for datasets with a smaller number of features compared to backward elimination\n",
    "\n",
    "Example in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00372f96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81eca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define forward selection function\n",
    "def forward_selection(X, y):\n",
    "    remaining_features = set(X.columns)\n",
    "    selected_features = []\n",
    "    current_score = 0.0\n",
    "    best_score = 0.0\n",
    "    \n",
    "    while remaining_features:\n",
    "        scores_with_candidates = []\n",
    "        for feature in remaining_features:\n",
    "            features_to_test = selected_features + [feature]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X[features_to_test], y, test_size=0.2, random_state=42)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = r2_score(y_test, y_pred)\n",
    "            scores_with_candidates.append((score, feature))\n",
    "        \n",
    "        # Select the feature with the best score\n",
    "        scores_with_candidates.sort(reverse=True)\n",
    "        best_score, best_feature = scores_with_candidates[0]\n",
    "        \n",
    "        if current_score < best_score:\n",
    "            remaining_features.remove(best_feature)\n",
    "            selected_features.append(best_feature)\n",
    "            current_score = best_score\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Apply forward selection\n",
    "best_features = forward_selection(X, y)\n",
    "print(\"Selected features:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab1ef6",
   "metadata": {},
   "source": [
    "LASSO\n",
    "LASSO is a type of regularization technique that both selects features and shrinks their coefficients. LASSO adds a penalty term—L1 regularization—to the cost function, which drives some feature coefficients to zero, effectively removing them from the model. This makes LASSO useful for automatic feature selection.\n",
    "\n",
    "How LASSO works\n",
    "L1 regularization\n",
    "The LASSO cost function is the ordinary least squares cost function with an added penalty term that is proportional to the absolute value of the feature coefficients. This penalty term shrinks some coefficients to zero.\n",
    "\n",
    "Cost Function\n",
    "=\n",
    "∑\n",
    "(\n",
    "y\n",
    "i\n",
    "−\n",
    "y\n",
    "^\n",
    "i\n",
    ")\n",
    "2\n",
    "+\n",
    "λ\n",
    "∑\n",
    "∣\n",
    "β\n",
    "j\n",
    "∣\n",
    "Cost Function=∑(y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "start text, C, o, s, t, space, F, u, n, c, t, i, o, n, end text, equals, sum, left parenthesis, y, start subscript, i, end subscript, minus, y, with, hat, on top, start subscript, i, end subscript, right parenthesis, squared, plus, lambda, sum, vertical bar, beta, start subscript, j, end subscript, vertical bar\n",
    "Where:\n",
    "\n",
    "yi  are the actual target values.\n",
    "\n",
    " ŷᵢ  are the predicted target values.\n",
    "\n",
    "βj  are the feature coefficients.\n",
    "\n",
    "λ  is the regularization parameter that controls the amount of shrinkage.\n",
    "\n",
    "Feature selection\n",
    "As the regularization parameter λ increases, more feature coefficients are driven to zero. Only the most significant features are left in the model.\n",
    "\n",
    "Advantages\n",
    "Automatically selects features by shrinking irrelevant feature coefficients to zero\n",
    "\n",
    "Helps prevent overfitting by penalizing large coefficients\n",
    "\n",
    "Works well with high-dimensional datasets where there are many features\n",
    "\n",
    "Disadvantages\n",
    "May remove features that are important in combination but not individually.\n",
    "\n",
    "The regularization parameter λ must be carefully tuned.\n",
    "\n",
    "Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf50366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Lasso model with alpha (λ) as the regularization parameter\n",
    "lasso_model = Lasso(alpha=0.01)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Display the coefficients of the features\n",
    "print(f\"Lasso Coefficients: {lasso_model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f13cfc",
   "metadata": {},
   "source": [
    "In this example, LASSO shrinks some feature coefficients to zero, effectively selecting only the most important features.\n",
    "\n",
    "Conclusion\n",
    "Feature selection is a critical step in building robust, interpretable, and efficient machine learning models. By using techniques like backward elimination, forward selection, and LASSO, you can reduce the number of features in your model, improve performance, and prevent overfitting. Each method has its own strengths and weaknesses, so choosing the right approach depends on the dataset and the problem at hand.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "Backward elimination removes the least significant features step by step.\n",
    "\n",
    "Forward selection adds the most significant features one by one.\n",
    "\n",
    "LASSO uses regularization to automatically select features by shrinking irrelevant ones to zero.\n",
    "\n",
    "Experimenting with these techniques will help you optimize your models for better performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8522b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22af64bc",
   "metadata": {},
   "source": [
    "# 2 - Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd15b31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = {\n",
    "    'StudyHours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'PrevExamScore': [30, 40, 45, 50, 60, 65, 70, 75, 80, 85],\n",
    "    'Pass': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['StudyHours', 'PrevExamScore']]\n",
    "y = df['Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the model (for the intercept)\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9306211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using Ordinary Least Squares (OLS) regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the summary, including p-values for each feature\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc15490",
   "metadata": {},
   "source": [
    "The goal is to start with all features and then progressively remove the least significant ones. The output will show a summary of the model, including the p-values for each feature. The p-value helps you determine the statistical significance of each feature: features with high p-values are considered less significant and should be removed.\n",
    "\n",
    "Step 5: Implement backward elimination\n",
    "The main idea behind backward elimination is to iteratively remove the feature with the highest p-value—greater than 0.05 in this case—and refit the model until all remaining features have a p-value less than 0.05.\n",
    "\n",
    "Step-by-step process:\n",
    "\n",
    "Fit the model with all features.\n",
    "\n",
    "Identify the feature with the highest p-value.\n",
    "\n",
    "Remove the feature with the highest p-value.\n",
    "\n",
    "Refit the model and repeat until all remaining features are statistically significant.\n",
    "\n",
    "Here’s a simple implementation of this process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f4b49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee240d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a significance level\n",
    "significance_level = 0.05\n",
    "\n",
    "# Perform backward elimination\n",
    "while True:\n",
    "    # Fit the model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Get the highest p-value in the model\n",
    "    max_p_value = model.pvalues.max()\n",
    "    \n",
    "    # Check if the highest p-value is greater than the significance level\n",
    "    if max_p_value > significance_level:\n",
    "        # Identify the feature with the highest p-value\n",
    "        feature_to_remove = model.pvalues.idxmax()\n",
    "        print(f\"Removing feature: {feature_to_remove} with p-value: {max_p_value}\")\n",
    "        \n",
    "        # Drop the feature\n",
    "        X = X.drop(columns=[feature_to_remove])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Display the final model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2658a",
   "metadata": {},
   "source": [
    "Step 6: Analyze the results\n",
    "Once you’ve completed the backward elimination process, review the final model summary. The remaining features should all have p-values less than the significance level, meaning they are statistically significant predictors of the target variable.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "Which features were removed during the backward elimination process?\n",
    "\n",
    "How did the model’s performance improve as irrelevant features were removed?\n",
    "\n",
    "Can you interpret the coefficients of the remaining features?\n",
    "\n",
    "Conclusion\n",
    "In this activity, you applied backward elimination to progressively remove the least significant features from a dataset. This technique helps simplify your model by keeping only the most relevant features, which can improve performance and reduce overfitting.\n",
    "\n",
    "Backward elimination is particularly useful when:\n",
    "\n",
    "You have many features, and not all are relevant.\n",
    "\n",
    "You want to improve model interpretability.\n",
    "\n",
    "You want to focus on the features that have the most impact on the target variable.\n",
    "\n",
    "Feel free to experiment with different datasets and adjust the significance level to explore how the feature selection process changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
