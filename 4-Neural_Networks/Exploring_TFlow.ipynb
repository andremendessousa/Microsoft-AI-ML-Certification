{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c23995e",
   "metadata": {},
   "source": [
    "Explanation of deep learning techniques\n",
    "Introduction\n",
    "Deep learning is a subset of machine learning that focuses on algorithms inspired by the structure and function of the brain, called artificial neural networks. These networks are designed to learn from vast amounts of data, making deep learning a powerful tool for tasks such as image recognition, natural language processing, and game playing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f1d0f",
   "metadata": {},
   "source": [
    "Key deep learning techniques\n",
    "Feedforward neural networks\n",
    "Overview\n",
    "A feedforward neural network (FNN) is the simplest form of neural network. In these networks, information flows in one direction—from the input layer, through the hidden layers, to the output layer—without any feedback loops.\n",
    "\n",
    "Key features\n",
    "Architecture: composed of an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "Activation functions: these introduce non-linearity into the network. Common activation functions include Rectified linear unit (ReLU) and Sigmoid.\n",
    "\n",
    "Training: feedforward networks are trained using backpropagation, where the error is propagated backward through the network to update the weights.\n",
    "\n",
    "Applications\n",
    "Image classification: image classification is the process of identifying and categorizing objects within an image, using algorithms that assign labels to specific features. For example, in a system that recognizes animals, an image classification model can distinguish between a cat, dog, or bird based on visual characteristics. A common real-world use is in medical imaging, where AI helps classify X-rays or MRIs as normal or showing signs of disease.\n",
    "\n",
    "Simple regression tasks: simple regression involves predicting a continuous value based on input variables, typically using linear regression to model the relationship. For example, predicting house prices based on factors such as square footage or location uses regression to estimate the expected price. A real-world example includes forecasting sales based on past data, helping businesses plan inventory or budget allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51291442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:53:13.652945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-10 10:53:14.404546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-07-10 10:53:14.404577: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-07-10 10:53:14.485246: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-10 10:53:16.889958: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-07-10 10:53:16.890206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-07-10 10:53:16.890227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062bffb",
   "metadata": {},
   "source": [
    "# 1 - FeedForward Neural Network - FNNs - Problemas mais simples - \"Substitui SciKit Learn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726c7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4b0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:54:21.805164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-07-10 10:54:21.806002: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-10 10:54:21.806041: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-dadea9): /proc/driver/nvidia/version does not exist\n",
      "2025-07-10 10:54:21.807275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the FNN model\n",
    "model_fnn = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3 output classes for the Iris dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb496c0",
   "metadata": {},
   "source": [
    "O modelo criado é uma Rede Neural Feedforward (FNN) projetada para classificar as flores do famoso conjunto de dados Iris em três espécies distintas. A arquitetura da rede é composta por três camadas densas (fully connected):\n",
    "\n",
    "- **Primeira camada densa:** Possui 64 neurônios e utiliza a função de ativação ReLU (Rectified Linear Unit), que introduz não-linearidade ao modelo e permite que ele aprenda relações complexas entre as variáveis de entrada. O input_shape=(X_train.shape[1],) indica que a camada espera receber vetores de 4 características (comprimento e largura das sépalas e pétalas).\n",
    "\n",
    "- **Segunda camada densa:** Com 32 neurônios e também ativação ReLU, essa camada aprofunda a capacidade de representação do modelo, permitindo que ele combine as informações extraídas na camada anterior de forma ainda mais abstrata.\n",
    "\n",
    "- **Camada de saída:** Possui 3 neurônios, correspondendo às três classes do problema (setosa, versicolor e virginica). A função de ativação softmax transforma as saídas em probabilidades, garantindo que a soma das probabilidades seja igual a 1. Assim, o modelo retorna a probabilidade de cada flor pertencer a cada uma das espécies.\n",
    "\n",
    "Matematicamente, cada camada realiza uma transformação linear dos dados de entrada (multiplicação por uma matriz de pesos e soma de um vetor de bias), seguida por uma função de ativação não-linear. O processo pode ser representado como:\n",
    "\n",
    "$$\n",
    "\\text{Saída} = \\text{softmax}(W_3 \\cdot \\text{ReLU}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot X + b_1) + b_2) + b_3)\n",
    "$$\n",
    "\n",
    "Onde $W_i$ e $b_i$ são os pesos e bias de cada camada, e $X$ é o vetor de entrada.\n",
    "\n",
    "Essa estrutura permite ao modelo aprender fronteiras de decisão complexas no espaço de características, tornando-o capaz de separar as diferentes espécies de flores com alta precisão. O uso de camadas densas e funções de ativação apropriadas é fundamental para capturar padrões não-lineares presentes nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c356d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 1s 47ms/step - loss: 0.9462 - accuracy: 0.6333 - val_loss: 0.8739 - val_accuracy: 0.7333\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8609 - accuracy: 0.6667 - val_loss: 0.8100 - val_accuracy: 0.7000\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8020 - accuracy: 0.6583 - val_loss: 0.7513 - val_accuracy: 0.7000\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7532 - accuracy: 0.6583 - val_loss: 0.7014 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7044 - accuracy: 0.6750 - val_loss: 0.6651 - val_accuracy: 0.8333\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6702 - accuracy: 0.9250 - val_loss: 0.6386 - val_accuracy: 0.8667\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6303 - accuracy: 0.8917 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5883 - accuracy: 0.9167 - val_loss: 0.5532 - val_accuracy: 0.7667\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5637 - accuracy: 0.6917 - val_loss: 0.5244 - val_accuracy: 0.7667\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5378 - accuracy: 0.8167 - val_loss: 0.5038 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5045 - accuracy: 0.9583 - val_loss: 0.4842 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4817 - accuracy: 0.9583 - val_loss: 0.4610 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4627 - accuracy: 0.9333 - val_loss: 0.4387 - val_accuracy: 0.9667\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4412 - accuracy: 0.9500 - val_loss: 0.4231 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4247 - accuracy: 0.9667 - val_loss: 0.4077 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4064 - accuracy: 0.9583 - val_loss: 0.3891 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3924 - accuracy: 0.9417 - val_loss: 0.3738 - val_accuracy: 0.9667\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3774 - accuracy: 0.9417 - val_loss: 0.3632 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3633 - accuracy: 0.9583 - val_loss: 0.3517 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3542 - accuracy: 0.9500 - val_loss: 0.3461 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b89cec73a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_fnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_fnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397faf6",
   "metadata": {},
   "source": [
    "# 2 - Implementando uma rede neural Convolucional - CNN - Imagens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448ae59",
   "metadata": {},
   "source": [
    "Convolutional neural networks\n",
    "Overview\n",
    "Convolutional neural networks (CNNs) are specialized for processing grid-like data such as images. CNNs use convolutional layers to detect patterns automatically in data, such as edges, textures, and shapes.\n",
    "\n",
    "Key features\n",
    "Convolutional layers: these layers apply filters (kernels) that slide over the input data, producing feature maps.\n",
    "\n",
    "Pooling layers: these layers reduce the spatial dimensions of the data, which decreases the computational load and helps the network focus on the most important features.\n",
    "\n",
    "Fully connected layers: these layers are usually at the end of the network to perform classification or regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baa551",
   "metadata": {},
   "source": [
    "Applications\n",
    "Image classification\n",
    "\n",
    "Object detection: object detection goes beyond classification by identifying and locating objects within an image, and drawing bounding boxes around them. For example, in self-driving cars, object detection is used to identify pedestrians, traffic lights, and other vehicles in real time. A common application is security surveillance, where cameras detect and track intruders in real time.\n",
    "\n",
    "Video analysis: video analysis processes video data to extract insights, such as recognizing actions, events, or patterns over time. For instance, in sports broadcasting, video analysis can track players' movements and analyze game strategies in real time. Another example is traffic monitoring, where video analysis is used to detect accidents or measure traffic flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9022668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 24s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf13637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dbe02",
   "metadata": {},
   "source": [
    "## Explicação conceitual e matemática da CNN construída\n",
    "\n",
    "A rede neural convolucional (CNN) montada no código é composta por várias camadas, cada uma responsável por uma transformação matemática específica dos dados de entrada. Vamos detalhar cada etapa, utilizando notação matricial e vetorial para descrever as operações.\n",
    "\n",
    "### 1. Entrada\n",
    "\n",
    "A entrada da rede é uma imagem de dimensão $32 \\times 32 \\times 3$, onde $32 \\times 32$ são as dimensões espaciais e $3$ representa os canais de cor (RGB).\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_0 \\in \\mathbb{R}^{32 \\times 32 \\times 3}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Primeira camada convolucional\n",
    "\n",
    "`Conv2D(32, (3, 3), activation='relu')`\n",
    "\n",
    "- Aplica 32 filtros (kernels) de tamanho $3 \\times 3$.\n",
    "- Cada filtro $\\mathbf{K}_i \\in \\mathbb{R}^{3 \\times 3 \\times 3}$.\n",
    "- A operação de convolução para cada filtro $i$ é:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_1^{(i)} = \\mathbf{X}_0 * \\mathbf{K}_i + b_i\n",
    "$$\n",
    "\n",
    "onde $*$ denota a convolução e $b_i$ é o viés do filtro $i$.\n",
    "\n",
    "- Após a convolução, aplica-se a função de ativação ReLU:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1^{(i)} = \\text{ReLU}(\\mathbf{Z}_1^{(i)}) = \\max(0, \\mathbf{Z}_1^{(i)})\n",
    "$$\n",
    "\n",
    "- A saída tem dimensão $30 \\times 30 \\times 32$ (considerando padding 'valid').\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Primeira camada de MaxPooling\n",
    "\n",
    "`MaxPooling2D((2, 2))`\n",
    "\n",
    "- Reduz as dimensões espaciais pela metade, pegando o valor máximo em cada janela $2 \\times 2$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_2^{(i)}(x, y) = \\max_{(m, n) \\in [0,1]^2} \\mathbf{A}_1^{(i)}(2x+m, 2y+n)\n",
    "$$\n",
    "\n",
    "- Saída: $15 \\times 15 \\times 32$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Segunda camada convolucional\n",
    "\n",
    "`Conv2D(64, (3, 3), activation='relu')`\n",
    "\n",
    "- 64 filtros de tamanho $3 \\times 3 \\times 32$.\n",
    "- Para cada filtro $j$:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_3^{(j)} = \\mathbf{A}_2 * \\mathbf{K}_j + b_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_3^{(j)} = \\text{ReLU}(\\mathbf{Z}_3^{(j)})\n",
    "$$\n",
    "\n",
    "- Saída: $13 \\times 13 \\times 64$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Segunda camada de MaxPooling\n",
    "\n",
    "`MaxPooling2D((2, 2))`\n",
    "\n",
    "- Reduz para $6 \\times 6 \\times 64$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_4^{(j)}(x, y) = \\max_{(m, n) \\in [0,1]^2} \\mathbf{A}_3^{(j)}(2x+m, 2y+n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Flatten\n",
    "\n",
    "`Flatten()`\n",
    "\n",
    "- Transforma o tensor $6 \\times 6 \\times 64$ em um vetor de dimensão $2304$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_5 \\in \\mathbb{R}^{2304}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Camada densa (fully connected)\n",
    "\n",
    "`Dense(64, activation='relu')`\n",
    "\n",
    "- Multiplicação matricial:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_6 = \\mathbf{W}_6 \\mathbf{a}_5 + \\mathbf{b}_6\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_6 = \\text{ReLU}(\\mathbf{z}_6)\n",
    "$$\n",
    "\n",
    "onde $\\mathbf{W}_6 \\in \\mathbb{R}^{64 \\times 2304}$, $\\mathbf{b}_6 \\in \\mathbb{R}^{64}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Camada de saída\n",
    "\n",
    "`Dense(10, activation='softmax')`\n",
    "\n",
    "- Multiplicação matricial:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_7 = \\mathbf{W}_7 \\mathbf{a}_6 + \\mathbf{b}_7\n",
    "$$\n",
    "\n",
    "- Softmax para obter probabilidades para cada classe $k$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_k = \\frac{\\exp(z_{7,k})}{\\sum_{l=1}^{10} \\exp(z_{7,l})}\n",
    "$$\n",
    "\n",
    "onde $\\mathbf{W}_7 \\in \\mathbb{R}^{10 \\times 64}$, $\\mathbf{b}_7 \\in \\mathbb{R}^{10}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumo do fluxo de dados\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_0 \\xrightarrow{\\text{Conv2D}} \\mathbf{A}_1 \\xrightarrow{\\text{MaxPool}} \\mathbf{A}_2 \\xrightarrow{\\text{Conv2D}} \\mathbf{A}_3 \\xrightarrow{\\text{MaxPool}} \\mathbf{A}_4 \\xrightarrow{\\text{Flatten}} \\mathbf{a}_5 \\xrightarrow{\\text{Dense}} \\mathbf{a}_6 \\xrightarrow{\\text{Dense+Softmax}} \\hat{\\mathbf{y}}\n",
    "$$\n",
    "\n",
    "Cada etapa transforma os dados, extraindo características espaciais (convoluções), reduzindo dimensionalidade (pooling), e finalmente classificando (camadas densas e softmax). O aprendizado ocorre ajustando os pesos ($\\mathbf{K}_i$, $\\mathbf{W}_6$, $\\mathbf{W}_7$) e vieses para minimizar a função de perda durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b980bd",
   "metadata": {},
   "source": [
    "# 3 - Recurrent Neural Network - Time Series - Voz - Dados financeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd965936",
   "metadata": {},
   "source": [
    "Recurrent neural networks\n",
    "Overview\n",
    "Recurrent neural networks (RNNs) are designed for sequential data, such as time series or language. Unlike FNNs, RNNs maintain a \"memory\" of previous inputs by passing the output of one layer back into the network.\n",
    "\n",
    "Key features\n",
    "Hidden state: this maintains the context of previous inputs in the network.\n",
    "\n",
    "Long short-term memory (LSTM) and gated recurrent units (GRUs): these are advanced RNN architectures that address the problem of long-term dependencies, making them effective at capturing information over long sequences.\n",
    "\n",
    "Applications\n",
    "Time-series forecasting\n",
    "\n",
    "Natural language processing, such as language translation and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc80fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic sine wave data\n",
    "t = np.linspace(0, 100, 10000)\n",
    "X = np.sin(t).reshape(-1, 1)\n",
    "\n",
    "# Prepare sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i+seq_length])\n",
    "        y_seq.append(data[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 200\n",
    "X_seq, y_seq = create_sequences(X, seq_length)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4bf2",
   "metadata": {},
   "source": [
    "O dataset foi criado para simular um problema típico de séries temporais, ideal para testar redes neurais recorrentes (RNNs). O processo matemático é o seguinte:\n",
    "\n",
    "1. **Geração do sinal senoide:**\n",
    "    - O vetor de tempo $t$ é criado igualmente espaçado entre 0 e 100, com 10.000 pontos:\n",
    "      $$\n",
    "      t = [0, \\Delta t, 2\\Delta t, ..., 100], \\quad \\Delta t = \\frac{100}{9999}\n",
    "      $$\n",
    "    - O sinal $X$ é então:\n",
    "      $$\n",
    "      X = \\sin(t)\n",
    "      $$\n",
    "      Cada elemento $X[i] = \\sin(t[i])$.\n",
    "\n",
    "2. **Criação das sequências para RNN:**\n",
    "    - Para treinar uma RNN, precisamos de pares (entrada, saída) onde a entrada é uma sequência de valores e a saída é o próximo valor da série.\n",
    "    - Para cada índice $i$ de $0$ até $N - \\text{seq\\_length} - 1$:\n",
    "      - Entrada: $[X[i], X[i+1], ..., X[i+\\text{seq\\_length}-1]]$\n",
    "      - Saída: $X[i+\\text{seq\\_length}]$\n",
    "    - Formalmente:\n",
    "      $$\n",
    "      \\mathbf{X}_{\\text{seq}}^{(i)} = [X[i], X[i+1], ..., X[i+\\text{seq\\_length}-1]]\n",
    "      $$\n",
    "      $$\n",
    "      y_{\\text{seq}}^{(i)} = X[i+\\text{seq\\_length}]\n",
    "      $$\n",
    "    - Isso gera um dataset de pares (sequência, próximo valor).\n",
    "\n",
    "3. **Divisão em treino e teste:**\n",
    "    - O conjunto de sequências $(\\mathbf{X}_{\\text{seq}}, y_{\\text{seq}})$ é dividido aleatoriamente em treino (80%) e teste (20%) usando `train_test_split`.\n",
    "\n",
    "**Resumo:**  \n",
    "O objetivo é prever o próximo valor da série senoide, dado os 100 valores anteriores. Assim, a RNN aprende a modelar padrões temporais do sinal senoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4fb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN\n",
    "model_rnn = models.Sequential([\n",
    "    layers.SimpleRNN(128, input_shape=(200, 1)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1d11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 8s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 2/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 3/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 4/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 5/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 6/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 7/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 8/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 9/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 10/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 11/15\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 12/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 13/15\n",
      "245/245 [==============================] - 8s 31ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 14/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n",
      "Epoch 15/15\n",
      "245/245 [==============================] - 7s 30ms/step - loss: 0.5124 - val_loss: 0.5151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b894585b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model_rnn.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fedaa",
   "metadata": {},
   "source": [
    "# 4 - Generative Adversarial Networks - GANs - Image - Recriar Dados - Sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4934ef1",
   "metadata": {},
   "source": [
    "Generative adversarial networks\n",
    "Overview\n",
    "Generative adversarial networks (GANs) consist of two networks, a generator and a discriminator, that are trained simultaneously. The generator creates fake data, and the discriminator attempts to distinguish between real and generated data. Over time, the generator improves its ability to produce realistic data.\n",
    "\n",
    "Key features\n",
    "Generator: learns to create data that is indistinguishable from real data.\n",
    "\n",
    "Discriminator: learns to distinguish between real and generated data.\n",
    "\n",
    "Adversarial training: the two networks compete with each other, leading to better results over time.\n",
    "\n",
    "Applications\n",
    "Image generation: image generation uses algorithms to create new images from scratch based on input data, often employing techniques such as GANs. For instance, AI can generate realistic images of faces that don’t belong to any real person. A common application is in art, where AI generates original artworks or designs based on style preferences.\n",
    "\n",
    "Style transfer: style transfer involves applying the artistic style of one image to the content of another, creating a blend of both. For example, AI can take a photo of a cityscape and apply the painting style of Van Gogh, transforming it into a starry, impressionistic version. This technique is used in digital art and content creation to stylize photos or videos.\n",
    "\n",
    "Data augmentation for training models: data augmentation artificially expands a dataset by creating modified versions of the existing data, such as by rotating, cropping, or flipping images. For instance, in image recognition tasks, augmenting a dataset of dog photos by flipping or zooming in on them helps improve the model's accuracy. This technique is essential for preventing overfitting in machine learning models and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(100,)),\n",
    "        layers.Dense(784, activation='sigmoid')  # Output: 28x28 flattened image\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(784,)),  # Input: Flattened 28x28 image\n",
    "        layers.Dense(1, activation='sigmoid')  # Output: Probability (real or fake)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998abffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess dataset (MNIST for example)\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Normalize images to [-1, 1] and flatten to (784,) for the discriminator input\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Normalize to range [-1, 1]\n",
    "X_train = X_train.reshape(-1, 784)  # Flatten 28x28 images to vectors of size 784\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(f\"X_train shape: {X_train.shape}\")  # Should print: (60000, 784)\n",
    "\n",
    "# Build the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create GAN model: stack generator and discriminator\n",
    "gan = models.Sequential([generator, discriminator])\n",
    "discriminator.trainable = False  # Freeze the discriminator when training the GAN\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "half_batch = batch_size // 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator with real images\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)  # Random real images\n",
    "    real_imgs = X_train[idx]\n",
    "    real_labels = np.ones((half_batch, 1))  # Real labels (1s)\n",
    "\n",
    "    # Train discriminator with fake images\n",
    "    noise = np.random.normal(0, 1, (half_batch, 100))  # Random noise input\n",
    "    fake_imgs = generator.predict(noise)  # Fake images generated by the generator\n",
    "    fake_labels = np.zeros((half_batch, 1))  # Fake labels (0s)\n",
    "\n",
    "    # Train the discriminator on real and fake images\n",
    "    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)\n",
    "\n",
    "    # Train the generator (the generator wants to fool the discriminator)\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))  # Generate new noise\n",
    "    gan_labels = np.ones((batch_size, 1))  # We want the generator to produce \"real\" images\n",
    "    g_loss = gan.train_on_batch(noise, gan_labels)\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss_real[0]}, Generator Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d2508",
   "metadata": {},
   "source": [
    "# 5 - Autoencoders - Unsupervised - Achar outliers - Compressão de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b44eb5",
   "metadata": {},
   "source": [
    "In this code, we train the discriminator on both real and fake images. Then, we train the generator to produce images that can fool the discriminator. The GAN is trained for 100 epochs, and we track the loss of both networks over time. For real-world applications, you’d want to train a GAN for well over 100 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441f19c",
   "metadata": {},
   "source": [
    "Autoencoders\n",
    "Overview\n",
    "Autoencoders are unsupervised learning models used for data compression. They consist of an encoder that compresses the input data into a lower-dimensional representation and a decoder that reconstructs the original data from this representation.\n",
    "\n",
    "Key features\n",
    "Encoder: compresses the data into a lower-dimensional space.\n",
    "\n",
    "Decoder: reconstructs the original data from the compressed representation.\n",
    "\n",
    "Bottleneck layer: this is the low-dimensional representation, also called the latent space.\n",
    "\n",
    "Applications\n",
    "Dimensionality reduction: dimensionality reduction reduces the number of features in a dataset while retaining essential information, simplifying the data for analysis. For example, using principal component analysis in a dataset with hundreds of features (such as gene expression data) can reduce it to a few key dimensions for easier visualization. This technique is often used in fields such as bioinformatics or finance to avoid overfitting and improve computational efficiency.\n",
    "\n",
    "Anomaly detection: anomaly detection identifies unusual patterns or data points that deviate from the norm, often used in monitoring and fraud detection. For example, in credit card transactions, anomaly detection algorithms flag suspicious activities such as unusually high purchases or transactions from uncommon locations. This method helps detect fraud, equipment malfunctions, or cybersecurity threats in real time.\n",
    "\n",
    "Data denoising: data denoising removes noise or irrelevant information from data to improve its quality and make it more usable for analysis. For instance, in image processing, denoising algorithms remove graininess or distortions in pictures captured under poor lighting conditions. It's commonly used in audio, video, and image processing to enhance the clarity and accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6415a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "def build_encoder():\n",
    "    input_img = layers.Input(shape=(784,))\n",
    "    encoded = layers.Dense(128, activation='relu')(input_img)\n",
    "    encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    return models.Model(input_img, encoded)\n",
    "\n",
    "# Define the decoder\n",
    "def build_decoder():\n",
    "    encoded_input = layers.Input(shape=(64,))\n",
    "    decoded = layers.Dense(128, activation='relu')(encoded_input)\n",
    "    decoded = layers.Dense(784, activation='sigmoid')(decoded)\n",
    "    return models.Model(encoded_input, decoded)\n",
    "\n",
    "# Build the full autoencoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "\n",
    "input_img = layers.Input(shape=(784,))\n",
    "encoded_img = encoder(input_img)\n",
    "decoded_img = decoder(encoded_img)\n",
    "\n",
    "autoencoder = models.Model(input_img, decoded_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea2c25",
   "metadata": {},
   "source": [
    "The encoder compresses the input image to a 64-dimensional latent space, while the decoder reconstructs the original 784-dimensional image. This compressed latent space is key to the autoencoder’s ability to learn meaningful representations of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9419a8",
   "metadata": {},
   "source": [
    "Training the Autoencoder\n",
    "Training the autoencoder involves minimizing the difference between the original input and the reconstructed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1088c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 11:19:48.340160: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2025-07-10 11:19:48.536656: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/235 [============================>.] - ETA: 0s - loss: 1.0116"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 784), found shape=(None, 28, 28)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m784\u001b[39m)  \u001b[38;5;66;03m# Flatten 28x28 images to vectors of size 784\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the autoencoder\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file0tvg7d6i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/envs/ai-tf/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 784), found shape=(None, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "# Normalize images to [-1, 1] and flatten to (784,) for the discriminator input\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Normalize to range [-1, 1]\n",
    "X_train = X_train.reshape(-1, 784)  # Flatten 28x28 images to vectors of size 784\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5ef2e",
   "metadata": {},
   "source": [
    "In this case, we use mean squared error (MSE) as the loss function since we want the output to be as close as possible to the original input. The model is trained for 50 epochs, and the performance is validated on a test set.\n",
    "\n",
    "Conclusion\n",
    "In this reading, we’ve implemented two powerful deep learning techniques—GANs and Autoencoders. GANs are used to generate new data, while Autoencoders help with data compression and reconstruction. Both of these models are critical in modern AI applications, from generating realistic images to reducing data dimensions. \n",
    "\n",
    "Mastering these models not only enhances your ability to handle complex data challenges but also opens up opportunities in fields like image synthesis, data augmentation, and anomaly detection.  \n",
    "\n",
    "Take the next step by experimenting with different datasets and tweaking these architectures to see how they perform. Start by choosing a dataset you're familiar with—perhaps images from your industry or text data from your field. Implement a basic GAN or Autoencoder on this dataset. Can you generate new, realistic data points or effectively compress and reconstruct your data? Remember, every experiment, successful or not, is a step towards mastering these powerful techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441af3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Autoencoder\n",
    "input_img = layers.Input(shape=(784,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = models.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d02bf1",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Deep learning techniques such as CNNs, RNNs, GANs, and autoencoders have revolutionized industries such as healthcare, finance, and entertainment. Each technique is suited to specific tasks, from image processing to sequential data analysis, and mastering these architectures is essential for solving complex problems with deep learning. These techniques form the backbone of today’s AI applications, and understanding when to use each model will help you become a more effective deep learning practitioner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
