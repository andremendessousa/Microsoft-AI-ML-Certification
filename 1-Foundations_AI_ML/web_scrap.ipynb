{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://data.hashrateindex.com/network-data/bitcoin-hashprice-index'  # Replace with the URL of the website you want to scrape\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print('Request successful!')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Parse the HTML content\n",
    "Once you’ve successfully retrieved the web page, use BeautifulSoup to parse the HTML content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashprice\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-table\u001b[39m\u001b[38;5;124m'\u001b[39m})  \u001b[38;5;66;03m# Replace 'data-table' with the actual id or class of the table\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract table rows\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Loop through the rows and extract data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Find the table containing the data\n",
    "table = soup.find('table', {'id': 'data-table'})  # Replace 'data-table' with the actual id or class of the table\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Loop through the rows and extract data\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Convert the data into a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data, columns=['Column1', 'Column2', 'Column3'])  # Replace with actual column names\n",
    "\n",
    "# Display the scraped data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cloud-computing comparison - Wikipedia\n",
      "                      Provider Launched Block storage Assignable IPs  \\\n",
      "0        Google Cloud Platform     2013           Yes             No   \n",
      "1  Oracle Cloud Infrastructure     2014           Yes            Yes   \n",
      "2          Amazon Web Services     2006           Yes            Yes   \n",
      "3                    IBM Cloud     2005           Yes            Yes   \n",
      "4              Microsoft Azure     2010           Yes            Yes   \n",
      "\n",
      "  SMTP support IOPS Guaranteed minimum Security  \\\n",
      "0        No[1]                     Yes   Yes[2]   \n",
      "1          Yes                     Yes   Yes[5]   \n",
      "2   Partial[6]                     Yes   Yes[7]   \n",
      "3        No[9]                     Yes  Yes[10]   \n",
      "4      Yes[11]                     Yes  Yes[12]   \n",
      "\n",
      "                                           Locations             Notes  \n",
      "0  br, ca, cl, us, be, ch, de, es, fi, it, po, nl...  SMTP blocked.[4]  \n",
      "1  us, ca, br, de, uk, nl, ch, in, aus, jp, kr, saud                    \n",
      "2  us, ca, br, ie, de, uk, cn, sg, au, jp, kr, in...   List of bugs[8]  \n",
      "3  us, gb, fr, de, nl, in, au, hk, kr, it, jp, no...                    \n",
      "4  ca, us, br, ie, nl, de, uk, cn, au, jp, in, kr...  List of bugs[13]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Send an HTTP request to the webpage\n",
    "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'  \n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(\"Title: \" + soup.title.text)\n",
    "\n",
    "# Find the table containing the data (selecting the first table by default)\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract headers from the first row (using <th> tags)\n",
    "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Loop through the rows and extract data (skip the first row with headers)\n",
    "data = []\n",
    "for row in rows[1:]:  # Start from the second row onwards\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     exit()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2: Find the document link\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m document_link \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownload-link\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Print the document link to verify\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument link found:\u001b[39m\u001b[38;5;124m'\u001b[39m, document_link)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Step 1: Send an HTTP request to the webpage\n",
    "url = 'https://archive.ics.uci.edu/dataset/45/heart+disease'  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage.')\n",
    "    exit()\n",
    "\n",
    "# Step 2: Find the document link\n",
    "document_link = soup.find('a', {'class': 'download-link'})['href']\n",
    "\n",
    "# Print the document link to verify\n",
    "print('Document link found:', document_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link de download encontrado: /auth/login\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL da página\n",
    "url = 'https://archive.ics.uci.edu/dataset/45/heart+disease'\n",
    "\n",
    "# Fazer a requisição HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200:\n",
    "    # Analisar o HTML da página\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Encontrar o link de download\n",
    "    download_link_element = soup.find('a', {'class': 'btn btn-primary btn-sm'})\n",
    "    if download_link_element:\n",
    "        download_link = download_link_element['href']\n",
    "        print(f'Link de download encontrado: {download_link}')\n",
    "    else:\n",
    "        print('Link de download não encontrado.')\n",
    "else:\n",
    "    print(f'Erro ao acessar a página: {response.status_code}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
